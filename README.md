# Parse, Align and Aggregate: Graph-driven Compositional Reasoning for Video Question Answering (QPVA $^3$)

<div align="center">

[ğŸ“ Project Structure](#-project-structure) â€¢ [ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“Š Benchmark](#-qpva-3bench) â€¢ [ğŸ¨ Visualization](#-visualization)

</div>

---

## ğŸŒŸ What is QPVA $^3$?

This paper introduces the QPVAÂ³ framework, a novel approach to Video Question-Answering (VideoQA) that enhances the transparency and verifiability of AI reasoning. Current models often provide answers without clear explanations. Our framework addresses this by breaking down a complex question into a compositional graph of simpler, interconnected sub-questions. It then aligns these sub-questions with relevant video clips to find answers and logically aggregates them into a final, coherent response. To evaluate our method, we also introduce QPVAÂ³Bench, a new benchmark dataset with detailed annotations, and novel metrics to measure reasoning consistency. Experiments show our framework significantly improves both accuracy and consistency, leading to a more transparent and trustworthy VideoQA system.

<div align="center">
  <img src="assets/img/QPVA3-framework.png" alt="QPVA framework" width="800">
</div>

## ğŸ“ Project Structure

Our project is adapted from [lmms-eval](https://github.com/EvolvingLMMs-Lab/lmms-eval)
We modified the project and extended it to suit the infreence pipeline of QPVA3.

```
QPVA3/
â”œâ”€â”€ assets/                   
â”‚   â””â”€â”€ image/
â”œâ”€â”€ QPVA3Bench/
â”‚   â”œâ”€â”€ visualization.py        # code to visualize our dataset
â”‚   â””â”€â”€ QPVA3Bench.json         # our dataset files
â”œâ”€â”€ lmm_eval/                 
â”‚   â”œâ”€â”€ lmms_eval/
â”‚   |   â”œâ”€â”€ models
â”‚   |   |   â”œâ”€â”€ chat            # new chat api, recommended
â”‚   |   |   |   â”œâ”€â”€ qpva3.py    # our QPVA3 model
|   |   |   |   â””â”€â”€ ...
â”‚   |   |   â”œâ”€â”€ simple          # the old simple api model, depreciated
â”‚   |   |   |   â”œâ”€â”€ qpva3.py    # our QPVA3 model
|   |   |   |   â””â”€â”€ ...         
â”‚   |   |   â”œâ”€â”€ model_utils
â”‚   |   |   |   â”œâ”€â”€ transformer_models   
â”‚   |   |   |   |  â”œâ”€â”€ qpva3.py # the transformer vertion of QPVA3 model
|   |   |   |   |  â””â”€â”€ ...
|   |   |   |   â””â”€â”€ ...
â”‚   |   â”œâ”€â”€ tasks
â”‚   |   |   â”œâ”€â”€ agqa_decomp     # task for evaluating AGQA-Decomp
â”‚   |   |   â”œâ”€â”€ qpva3_bench     # task for evaluating QPVA3Bench
|   |   |   â””â”€â”€ ...
|   |   â””â”€â”€ ...
â”‚   â””â”€â”€ ...                 
â””â”€â”€ requirements.txt           # Python package dependencies 
```

## ğŸš€ Quick Start

### 1. clone the repo
``` bash
git clone https://github.com/QPVA3/QPVA3-PAMI.git
```
install lmms-eval followingï¼šhttps://github.com/EvolvingLMMs-Lab/lmms-eval

install the requirements via
``` bash
pip install -r requirements.txt
```

### 2. Train the models
Download the train data from [AGQA-Decomp](https://agqa-decomp.cs.washington.edu/)

Fintune the models with [llama-factory](https://github.com/hiyouga/LLaMA-Factory)

### 3. Evaluate the models with lmms-eval
Run your trained model with the following command to test the performance on AGQA-Decomp.
```bash
lmms_eval --model qpva3 --model_args.answerer_path Qwen/Qwen2.5-VL-7B-Instruct --model_args.max_pixels 12845056 --model_args.attn_implementation flash_attention_2 --model_args.interleave_visuals False --tasks agqa_decomp --batch_size 1 --cache_requests true --verbosity=DEBUG
```

## ğŸ“Š QPVA $^3$?Bench
The QPVAÂ³Bench is stored under `QPVA3Bench` folder, in which the `QPVA3Bench.json` contains the dataset itself, while `visualization.py` provides a simple tool to visualize the  question structure of the dataset.

`QPVA3Bench.json` contains a json dict, where the keys are identical question keys representing the video-question pair, and the values are a dictionary containing the following contents:

- `id`: the id of this sample. Named as `{dataset_name}#{id_in_dataset}`, can be used to find the source of origial video-question pair.
- `video`: the path to video file. Named as `videos/{dataset_name}#{original_video_name_in_dataset}`, can be used to find the video corresponds to this video-question pair from the source dataset.
- `main_questions`: the main question of this sample, also the root of question decomposition tree.
- `main_answer`: the answer to the main question.
- `decomposition`: the decomposition tree, which is a nested dictionary of the following key-value pairs:
  - `question_id` the id of this quesition in the decomposition tree.
  - `question`: the current question,
  - `subquestions`: Optional, only exists when current question is not a leaf question. When exists, it is a list of nested dictionary with same structure, representing the decomposed questions of current question.
- `answers`: the answers to each question in the decomposition tree. Stored as a dict where the keys are `question_id` and values are corresponding answers.

The visualization scripe can be run by ```python visualization.py```. Input the index of the sample as  to visualize
  

## ğŸ¨ Visualization

### Full Reasoning Case
<div align="center">
  <img src="assets/img/visualization.png" alt="Sample Results Table 1" width="800">

*Qualitative example showcasing how QPVAÂ³ framework achieves successful successful in-depth analysis for VideoQA.*
</div>

* We first present an example that highlights compositional inconsistencies despite a correctly answered main question, demonstrating the capability of QPVA $^{3}$ to provide transparent analysis of MLLM outputs.
This case also serves to illustrate certain limitations of our approach.
Although VideoLLaMA3 accurately shows that the man spins the woman twice, it misinterprets the skating motion as dancing. 
While this perceptual error does not affect the recognition of the interaction between the man and the woman, it may yield unreliable responses for related sub-questions (*e.g.*, sub-question $A_5$), potentially impacting the user experience during conversational interactions.
Our framework exposes such potential failures in MLLMs; however, errors in spatial understanding remain challenging to correct, as the temporal video alignment mechanism offers limited support when nearly the entire video is deemed relevant (as seen in the alignment results for $I_1$ , $A_1$, $A_2$, and $A_5$).
Furthermore, since atomic questions form the basis for subsequent logical reasoning, errors at this initial stage are difficult for the reasoner to correct.
In the future, as the perceptual capabilities of MLLMs improve, our framework is expected to deliver more accurate and robust reasoning ability for these models.

### Conflict Reasoner Cases
<div align="center">
  <img src="assets/img/corr.png" alt="Sample Results Table 2" width="800">

  *Successful Case*
</div>

<div align="center">
  <img src="assets/img/err.png" alt="Sample Results Table 2" width="800">

  *Failure Case*
</div>

* We also present two cases showing how the reasoner handle conflict situations.
* In the successful case, the baseline model (VideoLLaMA3 + Aligner) incorrectly judges the cat's behavior as ``a bit anxious``. However, our reasoner aggregates evidence from atomic sub-questions, such as the cat's emotion being ``Friendly and Comfortable`` ($A_5$) and its action of sitting on the person. This creates a logical deduction that conflicts with the baseline's direct but incorrect answer. The reasoner successfully resolves this conflict by prioritizing the deduction derived from the detailed sub-question evidence, thus correcting the final answer to ``Friendly and Comfortable``. This case demonstrates the effectiveness of the first stage of conflict resolution, where aggregated evidence from sub-questions is used to challenge and override a flawed holistic judgment from the baseline model.
* Conversely, the failure case exposes a limitation in the second stage of conflict resolution. In this instance, the reasoner correctly aggregates the counts from the sub-questions (three black cats plus one orange cat) to arrive at the correct total of ``Four``. This correct aggregation creates a conflict with the baseline's incorrect direct answer, ``Three``. However, during the final answer selection step, the reasoner attempts to re-verify the count from the video but makes a perceptual error, failing to spot the orange cat. It consequently discards its own correct, logically derived answer (``Four``) and erroneously reverts to the incorrect answer (``Three``). This highlights a failure mode where the final self-correction mechanism itself is flawed, leading it to overturn a correct intermediate conclusion. This shows that while our two-stage process is powerful, its final output can still be compromised by errors in its own final verification step.
